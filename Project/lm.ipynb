{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\marce\\anaconda3\\lib\\site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import random\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import collections \n",
    "from ipynb.fs.full.corpus import *\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, n):\n",
    "    padded = []\n",
    "    final_lst = []\n",
    "    padding_process = list(pad_both_ends(tokens, n))\n",
    "    for i in padding_process:\n",
    "        # pad_both_ends adds to the sequence \"<s>\" from the beginning and \"</s>\" from the end. \n",
    "        # that's why I iterate through the list to find \"<s>\" and \"</s>\"\n",
    "        # created a new list \"padded\" to append the list padding_process but instead of \"<s>\" and \"</s>\",\n",
    "        # I append \"None\" to it\n",
    "        if i == \"<s>\" or i == \"</s>\":\n",
    "            padded.append('None')\n",
    "        else:\n",
    "            padded.append(i)\n",
    "    # here, instead of the list \"tokens\" I use \"padded\", because it contains both PADs and tokens\n",
    "    ngram = ngrams(padded, n)\n",
    "    # I iterate through the n-gram generator and append them to final_lst in order to return all n-grams\n",
    "    # otherwise, the code would return only one n-gram\n",
    "    for i in ngram:\n",
    "        final_lst.append(i)\n",
    "    return final_lst  \n",
    "#get_ngrams(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.3333333333333333,\n",
       " 'cat': 0.16666666666666666,\n",
       " 'dog': 0.16666666666666666,\n",
       " 'runs': 0.3333333333333333}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(word_counts):\n",
    "    #print(word_counts.values())\n",
    "    # creating new dictionary for the output\n",
    "    new_dict = {}\n",
    "    # summing up the values of word_counts dictionary\n",
    "    sum_of_values = sum(word_counts.values())\n",
    "    # iterating through word_count to access its keys and values\n",
    "    for k, v in word_counts.items():\n",
    "        # counting probability distribution of each word\n",
    "        result = v / sum_of_values\n",
    "        # adding keys and probability distribution results to the new dictionary\n",
    "        new_dict[k] = result\n",
    "    return new_dict\n",
    "normalize({'the':2, 'cat':1, 'dog':1, 'runs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    # convert to list for not getting TypeError\n",
    "    words = list(distribution.keys())\n",
    "    probabilities = distribution.values()\n",
    "    # returning randomly chosen word from words\n",
    "    # random.choices returns a word according to its probability distribution(that often)\n",
    "    random_value = random.choices(words, probabilities, k = 1)[0]\n",
    "    return random_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.counts = {}\n",
    "        self.vocabulary = set()\n",
    "        self.token_result = [] \n",
    "        self.all_ngrams = []\n",
    "        self.dict = {}\n",
    "        self.normalized_results = {}\n",
    "        self.tokens = []\n",
    "    def train(self, token_sequences):\n",
    "        # iterating through token_sequences in order to add the values to self.vocabulary\n",
    "        # since self.vocabulary shouldn't contain duplicates; I check if the value is already in self.vocabulary\n",
    "        # and if not, then I add it to self.vocabulary\n",
    "        #print(token_sequences)\n",
    "        for lst in token_sequences:\n",
    "            for el in lst:\n",
    "                if el not in self.vocabulary:\n",
    "                    self.vocabulary.add(el)\n",
    "                else:\n",
    "                    continue\n",
    "        # token_sequences does not contain None, that's why I additionaly add it to self.vocabulary\n",
    "                #self.vocabulary.add(None)\n",
    "            #print(self.vocabulary)\n",
    "        # here, I call get_ngrams function which takes lists of token_sequences and creates n-grams, \n",
    "        # based on the number assigned in self.n\n",
    "            n_grams = get_ngrams(lst, self.n)\n",
    "            #print(n_grams)\n",
    "        # creating one list of n-grams\n",
    "            for m in n_grams:\n",
    "                self.all_ngrams.append(m)\n",
    "            #print(self.all_ngrams)\n",
    "        # counting how many times each tuple appeared\n",
    "        # calling collection.Counter() and running it on all n-grams\n",
    "        value = collections.Counter(self.all_ngrams) \n",
    "        #print(value)\n",
    "        # iterating through the value\n",
    "        for tups in value: \n",
    "            # checking if [:-1] words of the tuple are already in self.counts\n",
    "            # if not, append them, [-1] word as the second key and the value as the value\n",
    "            if tups[:-1] not in self.counts.keys():\n",
    "                self.counts[tups[:-1]] = {}\n",
    "                self.counts[tups[:-1]][tups[-1]] = value[tups]\n",
    "            # if [:-1] are already there, add [-1] word to the existing tuple as the second key\n",
    "            # update the self.counts and return\n",
    "            elif tups[:-1] in self.counts.keys():\n",
    "                self.dict[tups[-1]] = value[tups] \n",
    "                self.counts[tups[:-1]].update(self.dict)\n",
    "        return self.counts\n",
    "    def p_next(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        #calling train() to tokenize, make a dict\n",
    "        new = self.train(tokens)\n",
    "        l = list(new.values())\n",
    "        merged_dict = {}\n",
    "        for this_dict in l:\n",
    "            for key,value in this_dict.items():\n",
    "                if key not in merged_dict:\n",
    "                    merged_dict[key] = value\n",
    "                elif key in merged_dict:\n",
    "                    merged_dict[key] = value + 1\n",
    "        norm = normalize(merged_dict)\n",
    "        final = {}\n",
    "        for key, val in norm.items():\n",
    "            for i in self.all_ngrams:\n",
    "                if key == i[-1]:\n",
    "                    final[key] = val\n",
    "        return final\n",
    "    def generate(self):\n",
    "        fin = []\n",
    "        pnext = self.p_next(self.tokens)\n",
    "        for i in pnext:\n",
    "            word = sample(pnext)\n",
    "            if word != 'None':\n",
    "                fin.append(word)\n",
    "            else:\n",
    "                fin.append(word)\n",
    "                break\n",
    "            \n",
    "        return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm = LanguageModel(3)\n",
    "#lm.train([[ ' the ' , ' cat ' , ' runs ' ],[' the ' , ' dog ' , ' runs ']])\n",
    "#lm.p_next(([[ ' the ' , ' cat ' , ' runs ' ],[' the ' , ' dog ' , ' runs ']]))\n",
    "#lm.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
